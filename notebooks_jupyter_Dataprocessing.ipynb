{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a78950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 12:32:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession instance\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"GCS Files Load\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2cb262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|                key|fare_amount|    pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-------------------+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|2009-06-15 17:26:21|        4.5|2009-06-15 17:26:21|      -73.844311|      40.721319|        -73.84161|       40.712278|              1|\n",
      "|2010-01-05 16:52:16|       16.9|2010-01-05 16:52:16|      -74.016048|      40.711303|       -73.979268|       40.782004|              1|\n",
      "|2011-08-18 00:35:00|        5.7|2011-08-18 00:35:00|      -73.982738|       40.76127|       -73.991242|       40.750562|              2|\n",
      "|2012-04-21 04:30:42|        7.7|2012-04-21 04:30:42|       -73.98713|      40.733143|       -73.991567|       40.758092|              1|\n",
      "|2010-03-09 07:51:00|        5.3|2010-03-09 07:51:00|      -73.968095|      40.768008|       -73.956655|       40.783762|              1|\n",
      "|2011-01-06 09:50:45|       12.1|2011-01-06 09:50:45|      -74.000964|       40.73163|       -73.972892|       40.758233|              1|\n",
      "|2012-11-20 20:35:00|        7.5|2012-11-20 20:35:00|      -73.980002|      40.751662|       -73.973802|       40.764842|              1|\n",
      "|2012-01-04 17:22:00|       16.5|2012-01-04 17:22:00|        -73.9513|      40.774138|       -73.990095|       40.751048|              1|\n",
      "|2012-12-03 13:10:00|        9.0|2012-12-03 13:10:00|      -74.006462|      40.726713|       -73.993078|       40.731628|              1|\n",
      "|2009-09-02 01:11:00|        8.9|2009-09-02 01:11:00|      -73.980658|      40.733873|        -73.99154|       40.758138|              2|\n",
      "|2012-04-08 07:30:50|        5.3|2012-04-08 07:30:50|      -73.996335|      40.737142|       -73.980721|       40.733559|              1|\n",
      "|2012-12-24 11:24:00|        5.5|2012-12-24 11:24:00|             0.0|            0.0|              0.0|             0.0|              3|\n",
      "|2009-11-06 01:04:03|        4.1|2009-11-06 01:04:03|      -73.991601|      40.744712|       -73.983081|       40.744682|              2|\n",
      "|2013-07-02 19:54:00|        7.0|2013-07-02 19:54:00|       -74.00536|      40.728867|       -74.008913|       40.710907|              1|\n",
      "|2011-04-05 17:11:05|        7.7|2011-04-05 17:11:05|      -74.001821|      40.737547|        -73.99806|       40.722788|              2|\n",
      "|2013-11-23 12:57:00|        5.0|2013-11-23 12:57:00|             0.0|            0.0|              0.0|             0.0|              1|\n",
      "|2014-02-19 07:22:00|       12.5|2014-02-19 07:22:00|       -73.98643|      40.760465|        -73.98899|       40.737075|              1|\n",
      "|2009-07-22 16:08:00|        5.3|2009-07-22 16:08:00|       -73.98106|       40.73769|       -73.994177|       40.728412|              1|\n",
      "|2010-07-07 14:52:00|        5.3|2010-07-07 14:52:00|      -73.969505|      40.784843|       -73.958732|       40.783357|              1|\n",
      "|2014-12-06 20:36:22|        4.0|2014-12-06 20:36:22|      -73.979815|      40.751902|       -73.979446|       40.755481|              1|\n",
      "+-------------------+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load form GCS\n",
    "gcs_path = \"gs://228bucket/train.csv\"\n",
    "\n",
    "# read CSV file\n",
    "df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(gcs_path)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65bd71c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: 55423856 rows, 8 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 12:47:01 ERROR TransportClient: Failed to send RPC RPC 8764269613026743268 to /10.128.0.11:43312: io.netty.channel.StacklessClosedChannelException\n",
      "io.netty.channel.StacklessClosedChannelException: null\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "24/05/12 12:47:01 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 9 from block manager BlockManagerId(12, cluster-9ee9-w-0.us-central1-f.c.data228project-423109.internal, 43329, None)\n",
      "java.io.IOException: Failed to send RPC RPC 8764269613026743268 to /10.128.0.11:43312: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392) ~[spark-network-common_2.12-3.5.0.jar:3.5.0]\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369) ~[spark-network-common_2.12-3.5.0.jar:3.5.0]\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:877) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:863) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:968) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:856) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:113) ~[netty-codec-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:881) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:863) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:968) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:856) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.write(IdleStateHandler.java:302) ~[netty-handler-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:879) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:940) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829) [?:?]\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n"
     ]
    }
   ],
   "source": [
    "row_count = df.count()\n",
    "column_count = len(df.columns)\n",
    "print(f\"DataFrame shape: {row_count} rows, {column_count} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5a0936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+---------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+---------------------+---------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "|            key_max|            key_min|fare_amount_max|fare_amount_min|pickup_datetime_max|pickup_datetime_min|pickup_longitude_max|pickup_longitude_min|pickup_latitude_max|pickup_latitude_min|dropoff_longitude_max|dropoff_longitude_min|dropoff_latitude_max|dropoff_latitude_min|passenger_count_max|passenger_count_min|\n",
      "+-------------------+-------------------+---------------+---------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+---------------------+---------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "|2015-06-30 23:59:54|2009-01-01 00:00:27|       93963.36|         -300.0|2015-06-30 23:59:54|2009-01-01 00:00:27|         3457.625683|        -3442.059565|        3408.789565|       -3492.263768|           3457.62235|         -3442.024565|         3537.132528|        -3547.886698|                208|                  0|\n",
      "+-------------------+-------------------+---------------+---------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+---------------------+---------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# extreme values\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "columns = df.columns\n",
    "expressions = {col: [F.max(col).alias(col + '_max'), F.min(col).alias(col + '_min')] for col in columns}\n",
    "aggregated_df = df.agg(*[expr for sublist in expressions.values() for expr in sublist])\n",
    "aggregated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4886b649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 13:18:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 12:======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "|summary|       fare_amount|  pickup_longitude|  pickup_latitude| dropoff_longitude| dropoff_latitude|   passenger_count|\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "|  count|          55423856|          55423856|         55423856|          55423480|         55423480|          55423856|\n",
      "|   mean|11.345045601663854|-72.50968444358728|39.91979178688818| -72.5112097297181|39.92068144482884|1.6853799201556816|\n",
      "| stddev|  20.7108321982325| 12.84888338140265|9.642353041994935|12.782196517830771|9.633345796415126|1.3276643570959683|\n",
      "|    min|            -300.0|      -3442.059565|     -3492.263768|      -3442.024565|     -3547.886698|                 0|\n",
      "|    max|          93963.36|       3457.625683|      3408.789565|        3457.62235|      3537.132528|               208|\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary_df = df.describe()\n",
    "summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b393c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|year|  count|\n",
      "+----+-------+\n",
      "|2009|8543166|\n",
      "|2010|8354997|\n",
      "|2011|8841346|\n",
      "|2012|8919666|\n",
      "|2013|8655190|\n",
      "|2014|8252682|\n",
      "|2015|3856809|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count in years\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_with_year = df.withColumn(\"year\", F.year(F.col(\"pickup_datetime\")))\n",
    "yearly_counts = df_with_year.groupBy(\"year\").count()\n",
    "yearly_counts = yearly_counts.orderBy(\"year\")\n",
    "yearly_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4c677bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|year|  count|\n",
      "+----+-------+\n",
      "|2010|8354997|\n",
      "|2011|8841346|\n",
      "|2012|8919666|\n",
      "|2013|8655190|\n",
      "|2014|8252682|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter out data from 2010 to 2014\n",
    "filtered_df = df_with_year.filter(F.col(\"year\").between(2010, 2014))\n",
    "\n",
    "# Group and count filtered data\n",
    "yearly_counts = filtered_df.groupBy(\"year\").count()\n",
    "yearly_counts.orderBy(\"year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87dbb32d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|                key|fare_amount|    pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-------------------+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|2010-01-05 16:52:16|       16.9|2010-01-05 16:52:16|      -74.016048|      40.711303|       -73.979268|       40.782004|              1|\n",
      "|2011-08-18 00:35:00|        5.7|2011-08-18 00:35:00|      -73.982738|       40.76127|       -73.991242|       40.750562|              2|\n",
      "|2012-04-21 04:30:42|        7.7|2012-04-21 04:30:42|       -73.98713|      40.733143|       -73.991567|       40.758092|              1|\n",
      "|2010-03-09 07:51:00|        5.3|2010-03-09 07:51:00|      -73.968095|      40.768008|       -73.956655|       40.783762|              1|\n",
      "|2011-01-06 09:50:45|       12.1|2011-01-06 09:50:45|      -74.000964|       40.73163|       -73.972892|       40.758233|              1|\n",
      "+-------------------+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Extract year from timestamp\n",
    "df_with_year = df.withColumn(\"year\", F.year(F.col(\"pickup_datetime\")))\n",
    "\n",
    "# Filter out data from 2010 to 2014\n",
    "years_of_interest = [2010, 2011, 2012, 2013, 2014]\n",
    "filtered_df = df_with_year.filter(F.col(\"year\").isin(years_of_interest))\n",
    "\n",
    "# Remove 'year' auxiliary column from results\n",
    "final_df = filtered_df.drop(\"year\")\n",
    "\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3626b3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|fare_amount|    pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|       16.9|2010-01-05 16:52:16|      -74.016048|      40.711303|       -73.979268|       40.782004|              1|\n",
      "|        5.7|2011-08-18 00:35:00|      -73.982738|       40.76127|       -73.991242|       40.750562|              2|\n",
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = final_df.drop(\"key\")\n",
    "final_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed33cd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|       fare_amount|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|   passenger_count|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|          43023881|          43023881|          43023881|          43023505|          43023505|          43023881|\n",
      "|   mean| 11.45069147179111|-72.38663522466115|39.844684480190836|-72.38069175554979| 39.84129464692027|1.6849287259789512|\n",
      "| stddev|13.704557609147619|13.801440280030436|10.631201080467019|13.741225332629819|10.626293480383582|1.3313331336418934|\n",
      "|    min|           -112.56|      -3442.059565|      -3492.263768|      -3442.024565|      -3547.886698|                 0|\n",
      "|    max|          61550.86|       3457.625683|       3408.789565|        3457.62235|       3537.132528|               208|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 35:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d52f109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after deletion DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "|summary|       fare_amount|  pickup_longitude|  pickup_latitude| dropoff_longitude| dropoff_latitude|   passenger_count|\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "|  count|          43022085|          43022085|         43022085|          43022085|         43022085|          43022085|\n",
      "|   mean|11.450657893499478|-72.38521253502451|39.84203364306933| -72.3807124762978|39.84034224936179|1.6849488349995125|\n",
      "| stddev|13.704575667893463|11.523495790997185|6.440936122161879|11.426139813424136|6.446743068257062|1.3313309664796518|\n",
      "|    min|           -112.56|      -3050.559458|       -80.880182|      -3050.559458|       -80.880182|                 0|\n",
      "|    max|          61550.86|       2228.738685|        89.742163|       2157.301527|        89.816665|               208|\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Delete rows in the pickup_latitude&dropoff_latitude column that are not in the range -90 to 90\n",
    "final_df_filtered = final_df.filter((col(\"pickup_latitude\") >= -90) & (col(\"pickup_latitude\") <= 90) & (col(\"dropoff_latitude\") >= -90) & (col(\"dropoff_latitude\") <= 90))\n",
    "\n",
    "print(\"\\nafter deletion DataFrame:\")\n",
    "final_df_filtered.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32645ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after deletion DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "|summary|       fare_amount|  pickup_longitude|  pickup_latitude| dropoff_longitude| dropoff_latitude|   passenger_count|\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "|  count|          43020789|          43020789|         43020789|          43020789|         43020789|          43020789|\n",
      "|   mean|11.450659353784879|-72.37262984888063|39.84209287433402|-72.37048475161347|39.84050055917293|1.6849332772581183|\n",
      "| stddev| 13.70467975423845|10.904398190067518|6.440626341324525|10.908544200149183|6.446145832009294|1.3312916902410423|\n",
      "|    min|           -112.56|       -168.603534|       -77.833874|        -173.95763|       -74.354612|                 0|\n",
      "|    max|          61550.86|        169.972765|        89.742163|        169.972765|        89.816665|               208|\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Delete rows in the pickup_longitude&dropoff_longitude column that are not in the range -180 to 180\n",
    "final_df_filtered2 = final_df_filtered.filter((col(\"pickup_longitude\") >= -180) & (col(\"pickup_longitude\") <= 180) & (col(\"dropoff_longitude\") >= -180) & (col(\"dropoff_longitude\") <= 180))\n",
    "\n",
    "print(\"\\nafter deletion DataFrame:\")\n",
    "final_df_filtered2.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "895d137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after deletion DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|      fare_amount|  pickup_longitude|  pickup_latitude| dropoff_longitude|  dropoff_latitude|   passenger_count|\n",
      "+-------+-----------------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|         43019680|          43019680|         43019680|          43019680|          43019680|          43019680|\n",
      "|   mean|11.44945041641536|-72.37267830493542|39.84211884743914|-72.37055213498593| 39.84053689331871|1.6849273402312617|\n",
      "| stddev|9.880095848122915| 10.90424559085245|6.440558841611602|10.908330430495125|6.4460470707882935|1.3312888325130892|\n",
      "|    min|              0.0|       -168.603534|       -77.833874|        -173.95763|        -74.354612|                 0|\n",
      "|    max|           1564.5|        169.972765|        89.742163|        169.972765|         89.816665|               208|\n",
      "+-------+-----------------+------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df_filtered3 = final_df_filtered2.filter((col(\"fare_amount\") >= 0) & (col(\"fare_amount\") <= 2000))\n",
    "\n",
    "print(\"\\nafter deletion DataFrame:\")\n",
    "final_df_filtered3.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e2f2133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after deletion DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "|summary|      fare_amount|  pickup_longitude|  pickup_latitude| dropoff_longitude| dropoff_latitude|   passenger_count|\n",
      "+-------+-----------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "|  count|         42826493|          42826493|         42826493|          42826493|         42826493|          42826493|\n",
      "|   mean|11.46060764560048|-72.37126634451855|39.84122739241672| -72.3694076343133|39.83978624861877|1.6925279172403866|\n",
      "| stddev|9.891615592207698|10.909384908586414|6.445068729955861|10.912602527780848|6.450125250236515|1.3294587658656984|\n",
      "|    min|              0.0|       -168.603534|       -77.833874|        -173.95763|       -74.354612|                 1|\n",
      "|    max|           1564.5|        169.972765|        89.742163|        169.972765|        89.816665|               208|\n",
      "+-------+-----------------+------------------+-----------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df_filtered4 = final_df_filtered3.filter((col(\"passenger_count\") > 0))\n",
    "\n",
    "print(\"\\nafter deletion DataFrame:\")\n",
    "final_df_filtered4.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf195dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#save file\n",
    "output_path = 'gs://228bucket/processed_train_1.0.csv'\n",
    "\n",
    "final_df_filtered4.write.mode('overwrite').option('header', 'true').csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26214e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:=====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|       fare_amount|  pickup_longitude|  pickup_latitude|dropoff_longitude|  dropoff_latitude|   passenger_count|\n",
      "+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|          42826493|          42826493|         42826493|         42826493|          42826493|          42826493|\n",
      "|   mean|11.460607645597449|-72.37126634451735|39.84122739241672|-72.3694076343127|39.839786248618445|1.6925279172403866|\n",
      "| stddev| 9.891615592207685|10.909384908586341|6.445068729955884|10.91260252778086| 6.450125250236519|1.3294587658656971|\n",
      "|    min|               0.0|       -168.603534|       -77.833874|       -173.95763|        -74.354612|                 1|\n",
      "|    max|            1564.5|        169.972765|        89.742163|       169.972765|         89.816665|               208|\n",
      "+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#check saved file\n",
    "#load form GCS\n",
    "new_path = \"gs://228bucket/processed_train_combined.csv\"\n",
    "\n",
    "# read CSV file\n",
    "df_new = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(new_path)\n",
    "\n",
    "df_new.describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}