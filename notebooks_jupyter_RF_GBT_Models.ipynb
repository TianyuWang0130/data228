{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba6ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col, unix_timestamp, dayofweek, hour, month, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1b5388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/15 11:25:16 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/05/15 11:25:16 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/05/15 11:25:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/05/15 11:25:16 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 41606)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RF and GBT Model\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26176e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = spark.read.csv('gs://228bucket/processed_train_2.0.csv/processed_dataset_2.0.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8593d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(withReplacement=False, fraction=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15923cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4283391"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f9ab32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05723a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 11:28:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:======================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|      fare_amount|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|   passenger_count|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|          4283391|           4283391|           4283391|           4283391|           4283391|           4283391|\n",
      "|   mean|11.45865025863937|-72.37123187140257|39.841392686071345|-72.36926431473576|39.839520175316125|1.6920607994927384|\n",
      "| stddev|9.901330284884985|10.910671410207991|  6.44615301071534|10.914090016025693| 6.452158642910057|1.3258650378198835|\n",
      "|    min|              0.0|       -128.107047|        -74.031213|       -121.925747|        -74.177085|                 1|\n",
      "|    max|            900.0|        148.498615|          81.48045|        148.498615|         83.283332|               208|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c372e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime and extract features\n",
    "data = data.withColumn(\"pickup_datetime\", unix_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSX\").cast(\"timestamp\"))\n",
    "data = data.withColumn(\"pickup_hour\", hour(col(\"pickup_datetime\")))\n",
    "data = data.withColumn(\"day_of_week\", dayofweek(col(\"pickup_datetime\")))\n",
    "data = data.withColumn(\"month\", month(col(\"pickup_datetime\")))\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    if lat1 == lat2 and lon1 == lon2:\n",
    "        return 0.0\n",
    "    else:\n",
    "        R = 6371.0  # Radius of the Earth in kilometers\n",
    "        lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])  # Convert degrees to radians\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        distance = R * c\n",
    "        return float(distance)\n",
    "\n",
    "udf_calculate_distance = udf(calculate_distance, FloatType())\n",
    "data = data.withColumn(\"distance\", udf_calculate_distance(col(\"pickup_latitude\"), col(\"pickup_longitude\"), col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a208c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+-----------+-----------+-----+---------+\n",
      "|fare_amount|    pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|pickup_hour|day_of_week|month| distance|\n",
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+-----------+-----------+-----+---------+\n",
      "|        9.0|2012-12-03 13:10:00|      -74.006462|      40.726713|       -73.993078|       40.731628|              1|         13|          2|   12|1.2532315|\n",
      "|       10.5|2010-09-07 13:18:00|      -73.985382|      40.747858|       -73.978377|        40.76207|              1|         13|          3|    9|1.6868613|\n",
      "|        4.9|2010-12-06 12:29:00|      -74.000632|      40.747473|       -73.986672|       40.740577|              1|         12|          2|   12|1.4039582|\n",
      "|        7.0|2014-05-01 09:12:00|      -73.966203|        40.7675|       -73.980915|        40.77424|              6|          9|          5|    5| 1.447958|\n",
      "|       16.5|2014-12-08 16:00:01|        -73.9828|      40.745287|        -74.01425|       40.702935|              1|         16|          2|   12|5.4038725|\n",
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+-----------+-----------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb08d0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- distance: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e944f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"distance\", \"pickup_hour\", \"day_of_week\", \"month\"]\n",
    "# Assemble the feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Split the data into training and testing sets (70% training, 30% testing)\n",
    "(training_data, testing_data) = data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5be3f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Fit the MinMaxScaler to the training data\n",
    "scaler_model = scaler.fit(training_data)\n",
    "\n",
    "# Transform the training data\n",
    "training_data_scaled = scaler_model.transform(training_data)\n",
    "\n",
    "# Transform the testing data\n",
    "testing_data_scaled = scaler_model.transform(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9620e",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f60e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/lib/spark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data = 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the Random Forest model\n",
    "rf = RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=\"fare_amount\", numTrees=100, seed=42)\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = rf.fit(training_data_scaled)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = rf_model.transform(testing_data_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"fare_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {:.2f}\".format(rmse))\n",
    "\n",
    "# Evaluate the model - R2\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"fare_amount\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(\"R-squared (R2) on test data = {:.2f}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7723f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.58\n",
      "Normalized RMSE on test data = 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the range of the label variable\n",
    "max_fare = predictions.agg({\"fare_amount\": \"max\"}).collect()[0][0]\n",
    "min_fare = predictions.agg({\"fare_amount\": \"min\"}).collect()[0][0]\n",
    "fare_range = max_fare - min_fare\n",
    "\n",
    "# Normalize RMSE\n",
    "normalized_rmse = rmse / fare_range\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {:.2f}\".format(rmse))\n",
    "print(\"Normalized RMSE on test data = {:.4f}\".format(normalized_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8340b70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "#save model\n",
    "model_path = \"gs://228bucket/Models/rf_model\"\n",
    "\n",
    "rf_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78214e39",
   "metadata": {},
   "source": [
    "# GBT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a858f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 12:03:25 WARN YarnAllocator: Container from a bad node: container_1715754161652_0010_01_000016 on host: cluster-789c-w-1.us-central1-f.c.vertical-kayak-423108-t5.internal. Exit status: 143. Diagnostics: [2024-05-15 12:03:25.089]Container killed on request. Exit code is 143\n",
      "[2024-05-15 12:03:25.089]Container exited with a non-zero exit code 143. \n",
      "[2024-05-15 12:03:25.090]Killed by external signal\n",
      ".\n",
      "24/05/15 12:03:25 ERROR YarnScheduler: Lost executor 15 on cluster-789c-w-1.us-central1-f.c.vertical-kayak-423108-t5.internal: Container from a bad node: container_1715754161652_0010_01_000016 on host: cluster-789c-w-1.us-central1-f.c.vertical-kayak-423108-t5.internal. Exit status: 143. Diagnostics: [2024-05-15 12:03:25.089]Container killed on request. Exit code is 143\n",
      "[2024-05-15 12:03:25.089]Container exited with a non-zero exit code 143. \n",
      "[2024-05-15 12:03:25.090]Killed by external signal\n",
      ".\n",
      "24/05/15 12:03:25 WARN TaskSetManager: Lost task 0.0 in stage 1047.0 (TID 24576) (cluster-789c-w-1.us-central1-f.c.vertical-kayak-423108-t5.internal executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container from a bad node: container_1715754161652_0010_01_000016 on host: cluster-789c-w-1.us-central1-f.c.vertical-kayak-423108-t5.internal. Exit status: 143. Diagnostics: [2024-05-15 12:03:25.089]Container killed on request. Exit code is 143\n",
      "[2024-05-15 12:03:25.089]Container exited with a non-zero exit code 143. \n",
      "[2024-05-15 12:03:25.090]Killed by external signal\n",
      ".\n",
      "24/05/15 12:03:25 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 15 for reason Container from a bad node: container_1715754161652_0010_01_000016 on host: cluster-789c-w-1.us-central1-f.c.vertical-kayak-423108-t5.internal. Exit status: 143. Diagnostics: [2024-05-15 12:03:25.089]Container killed on request. Exit code is 143\n",
      "[2024-05-15 12:03:25.089]Container exited with a non-zero exit code 143. \n",
      "[2024-05-15 12:03:25.090]Killed by external signal\n",
      ".\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data = 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1054:===================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.88\n",
      "Normalized RMSE on test data = 0.0103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the GBT model\n",
    "gbt = GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=\"fare_amount\", maxIter=100, seed=42)\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, gbt])\n",
    "\n",
    "# Train the GBT model\n",
    "gbt_model = gbt.fit(training_data_scaled)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "gbt_predictions = gbt_model.transform(testing_data_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "gbt_evaluator = RegressionEvaluator(labelCol=\"fare_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "gbt_rmse = evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "# Evaluate the model - R2\n",
    "gbt_evaluator_r2 = RegressionEvaluator(labelCol=\"fare_amount\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = gbt_evaluator_r2.evaluate(gbt_predictions)\n",
    "print(\"R-squared (R2) on test data = {:.2f}\".format(r2))\n",
    "\n",
    "# Calculate the range of the label variable\n",
    "max_fare_gbt = gbt_predictions.agg({\"fare_amount\": \"max\"}).collect()[0][0]\n",
    "min_fare_gbt = gbt_predictions.agg({\"fare_amount\": \"min\"}).collect()[0][0]\n",
    "fare_range_gbt = max_fare_gbt - min_fare_gbt\n",
    "\n",
    "# Normalize RMSE for GBT\n",
    "normalized_rmse_gbt = gbt_rmse / fare_range_gbt\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {:.2f}\".format(gbt_rmse))\n",
    "print(\"Normalized RMSE on test data = {:.4f}\".format(normalized_rmse_gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b07188d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Define the path to save the model\n",
    "model_path = \"gs://228bucket/Models/gbt_model\"\n",
    "\n",
    "# Save the model\n",
    "gbt_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd97bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}