{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/15 22:55:08 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/05/15 22:55:08 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/05/15 22:55:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/05/15 22:55:08 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DropOffCoordinates\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col, unix_timestamp, dayofweek, hour, month, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the data\n",
    "data = spark.read.csv('gs://228bucket/processed_train_2.0.csv/processed_dataset_2.0.csv', header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42826493"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.sample(withReplacement=False, fraction=0.1, seed=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4283040"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|fare_amount|    pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|        5.3|2010-07-07 14:52:00|      -73.969505|      40.784843|       -73.958732|       40.783357|              1|\n",
      "|       10.5|2010-09-07 13:18:00|      -73.985382|      40.747858|       -73.978377|        40.76207|              1|\n",
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+-----------+---------------------+--------------------+\n",
      "|fare_amount|    pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|pickup_time|seconds_from_midnight|            features|\n",
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+-----------+---------------------+--------------------+\n",
      "|        5.3|2010-07-07 14:52:00|      -73.969505|      40.784843|       -73.958732|       40.783357|              1|   14:52:00|                53520|[53520.0,-73.9695...|\n",
      "|       10.5|2010-09-07 13:18:00|      -73.985382|      40.747858|       -73.978377|        40.76207|              1|   13:18:00|                47880|[47880.0,-73.9853...|\n",
      "|        7.3|2011-06-21 16:15:00|      -73.991875|      40.754437|        -73.97723|       40.774323|              3|   16:15:00|                58500|[58500.0,-73.9918...|\n",
      "|        3.3|2011-12-14 07:53:00|      -73.988877|      40.763577|       -73.985573|       40.760262|              1|   07:53:00|                28380|[28380.0,-73.9888...|\n",
      "|        6.9|2010-05-14 08:09:00|      -73.955118|      40.768942|       -73.963495|       40.761887|              2|   08:09:00|                29340|[29340.0,-73.9551...|\n",
      "|        7.5|2014-05-19 06:26:00|      -73.990815|      40.730495|       -73.978337|       40.751085|              1|   06:26:00|                23160|[23160.0,-73.9908...|\n",
      "|        7.7|2011-02-22 11:09:16|      -73.968881|      40.764533|       -73.985311|       40.748956|              1|   11:09:16|                40156|[40156.0,-73.9688...|\n",
      "|       14.1|2011-03-09 07:24:34|      -73.974492|      40.751369|       -74.010585|       40.703364|              1|   07:24:34|                26674|[26674.0,-73.9744...|\n",
      "|        7.7|2011-08-04 10:38:00|      -73.991607|       40.76318|       -73.982718|        40.76239|              2|   10:38:00|                38280|[38280.0,-73.9916...|\n",
      "|       35.0|2012-12-06 18:05:00|       -73.95331|      40.787772|       -73.944352|       40.719772|              2|   18:05:00|                65100|[65100.0,-73.9533...|\n",
      "|       14.5|2010-03-27 02:26:39|      -73.961598|      40.716487|       -73.960417|       40.674963|              2|   02:26:39|                 8799|[8799.0,-73.96159...|\n",
      "|        5.5|2013-01-26 18:36:00|      -73.950305|      40.785607|       -73.943587|       40.789532|              1|   18:36:00|                66960|[66960.0,-73.9503...|\n",
      "|       17.0|2013-02-08 09:50:04|      -73.962458|      40.759027|       -73.986199|       40.753397|              1|   09:50:04|                35404|[35404.0,-73.9624...|\n",
      "|       15.3|2011-06-12 13:33:00|      -73.949477|      40.768018|       -74.003118|       40.733022|              5|   13:33:00|                48780|[48780.0,-73.9494...|\n",
      "|        7.0|2012-11-11 18:23:00|      -73.999998|      40.727087|       -73.992448|       40.735602|              1|   18:23:00|                66180|[66180.0,-73.9999...|\n",
      "|       42.5|2011-01-10 16:07:00|      -73.795677|      40.807722|       -73.978683|       40.724365|              2|   16:07:00|                58020|[58020.0,-73.7956...|\n",
      "|       16.5|2013-09-07 22:29:00|        -73.9952|      40.731697|        -73.94977|       40.780347|              1|   22:29:00|                80940|[80940.0,-73.9952...|\n",
      "|        7.3|2011-02-27 14:45:00|      -73.983758|       40.74643|       -73.963557|       40.774088|              1|   14:45:00|                53100|[53100.0,-73.9837...|\n",
      "|       14.9|2012-01-13 11:28:00|      -73.955268|      40.782767|       -73.932382|       40.794812|              6|   11:28:00|                41280|[41280.0,-73.9552...|\n",
      "|        7.0|2012-12-15 19:11:53|      -74.002462|      40.718762|       -74.007071|       40.705536|              2|   19:11:53|                69113|[69113.0,-74.0024...|\n",
      "+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+-----------+---------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.functions import from_unixtime, unix_timestamp, col\n",
    "# split the time string into hours, minutes and seconds\n",
    "df = df.withColumn('pickup_time', from_unixtime(unix_timestamp(col('pickup_datetime'), 'yyyy-MM-dd HH:mm:ss'), 'HH:mm:ss'))\n",
    "time_split = split(col('pickup_time'), ':')\n",
    "\n",
    "# Calculate the total number of seconds since midnight\n",
    "seconds_from_midnight = (time_split.getItem(0).cast(\"int\") * 3600) + (time_split.getItem(1).cast(\"int\") * 60) + time_split.getItem(2).cast(\"int\")\n",
    "\n",
    "# Add new column to DataFrame\n",
    "df = df.withColumn('seconds_from_midnight', seconds_from_midnight)\n",
    "\n",
    "# Eigenvectors can now be created using new numeric columns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['seconds_from_midnight', 'pickup_longitude', 'pickup_latitude', 'passenger_count'],\n",
    "    outputCol='features'\n",
    ")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target variables\n",
    "data = df.select('features', 'dropoff_longitude', 'dropoff_latitude')\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the model for longitude\n",
    "rf_longitude = RandomForestRegressor(featuresCol='features', labelCol='dropoff_longitude', numTrees=10, seed=42)\n",
    "model_longitude = rf_longitude.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/lib/spark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_longitude = model_longitude.transform(test_data)\n",
    "evaluator_longitude = RegressionEvaluator(labelCol='dropoff_longitude', predictionCol='prediction', metricName='rmse')\n",
    "rmse_longitude = evaluator_longitude.evaluate(predictions_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (Longitude): 3.500806667538185\n"
     ]
    }
   ],
   "source": [
    "print(\"Root Mean Squared Error (Longitude):\", rmse_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "#save model\n",
    "model_path_lg = \"gs://228bucket/Models/lgfinal\"\n",
    "model_longitude.write().overwrite().save(model_path_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 23:25:31 WARN BlockManager: Asked to remove block broadcast_71_piece0, which does not exist\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the model for latitude\n",
    "rf_latitude = RandomForestRegressor(featuresCol='features', labelCol='dropoff_latitude', numTrees=10, seed=42)\n",
    "model_latitude = rf_latitude.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:===================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (Latitude): 2.5666384925377153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Predicting drop-off latitude\n",
    "predictions_latitude = model_latitude.transform(test_data)\n",
    "evaluator_latitude = RegressionEvaluator(labelCol='dropoff_latitude', predictionCol='prediction', metricName='rmse')\n",
    "rmse_latitude = evaluator_latitude.evaluate(predictions_latitude)\n",
    "print(\"Root Mean Squared Error (Latitude):\", rmse_latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "model_path_lat = \"gs://228bucket/Models/latfinal\"\n",
    "model_latitude.write().overwrite().save(model_path_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}